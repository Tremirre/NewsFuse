{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tf_hub\n",
    "import wandb\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import nltk\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "from train_wandb import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = pd.read_excel(\"../../data/labeled_dataset.xlsx\").rename(columns={\"Unnamed: 0\": \"id\"})\n",
    "minified_df = labeled_df[[\"sentence\", \"Label_opinion\"]]\n",
    "minified_df[\"target\"] = (minified_df[\"Label_opinion\"] == \"Expresses writer’s opinion\").astype(int)\n",
    "minified_df = minified_df.drop(\"Label_opinion\", axis=1)\n",
    "minified_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpqa_df = pd.read_csv(\"../../data/mpqa_filtered.csv\", sep = \";\").rename(columns={\"sent\": \"sentence\"})\n",
    "mpqa_df[\"target\"] = np.where(mpqa_df['score'] >= 2, 1, 0)\n",
    "mpqa_df.drop(columns=[\"score\"], inplace = True)\n",
    "mpqa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_df = pd.concat([minified_df, mpqa_df])\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (full_dataset_df['sentence'].values, full_dataset_df['target'].values)\n",
    "    )\n",
    "    .shuffle(full_dataset_df.shape[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_opinion_text = \"\"\"\n",
    "Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, \n",
    "and artificial intelligence concerned with the interactions between computers and human language, \n",
    "in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "The goal is a computer capable of understanding the contents of documents, \n",
    "including the contextual nuances of the language within them. The technology can then accurately extract \n",
    "information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_text = \"\"\"\n",
    "One of my favourite films is Titanic. I`m a great fan of romantic movies and I`m very keen on the history of the tragic Titanic. \n",
    "The movie tells the dramatic story of the Titanic with Leonardo Di Caprio and Kate Winslet as the main actors.\n",
    "In the year 1912 a young poor guy, Leonardo, travels by the gorgeous ship Titanic from London to New York with a big dream. \n",
    "On board he meets a fabulous, wealthy girl, Kate Winslet. He fells in love with her as soon as he sees this gorgeous girl. \n",
    "However, after a few days the ship hits an enormous iceberg and the tragedy begins to unfold. The Titanic starts sinking…\n",
    "The movie is very close to the real tragedy. It shows a beautiful love story with an extremely sad ending. \n",
    "The acting is first-rate. Leonardo Di Caprio`s and Kate Winslet`s performance is so brilliant that I cried during the whole movie.\n",
    "This movie brings a tear to your eyes. If you want to cry and melt down to a glamurous love story, \n",
    "I strongly recommend that you watch this movie. Titanic is well worth seeing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(preprocess_layer, encoder_layer):\n",
    "    input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"input\")\n",
    "    preprocessing_layer = preprocess_layer(input_layer)\n",
    "    encoder_outputs = encoder_layer(preprocessing_layer)\n",
    "    pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "    output_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(pooled_output)\n",
    "    model = tf.keras.Model(input_layer, output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_experiment(config, name):\n",
    "    \n",
    "    preprocess_layer = tf_hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "        name=\"preprocessing\"\n",
    "    )\n",
    "\n",
    "    encoder_layer = tf_hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/2\",\n",
    "        name=\"BERT_encoder\",\n",
    "        trainable=True\n",
    "    )\n",
    "\n",
    "    model = build_model(preprocess_layer, encoder_layer)\n",
    "    model.summary()\n",
    "\n",
    "    trainer = Trainer(config,model,dataset, name)\n",
    "    trainer.compile_model()\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    trainer.test_model()\n",
    "    trainer.predict_model(no_opinion_text, \"no opinion text\")\n",
    "    trainer.predict_model(opinion_text, \"opinion text\")\n",
    "\n",
    "    def get_probabilities(text):\n",
    "        prob = model.predict([text])\n",
    "        return np.hstack([1 - prob, prob])\n",
    "\n",
    "    explainer = LimeTextExplainer(class_names=[\"Non-biased\", \"Biased\"])\n",
    "    explanation_2 = trainer.explain_prediction(no_opinion_text, \"No opinion text explanation\")\n",
    "    explanation_1 = trainer.explain_prediction(opinion_text,  \"Opinion text explanation\")\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        explanation_1.show_in_notebook(text=True)\n",
    "        explanation_2.show_in_notebook(text=True)\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"architecture\": \"bigger-BERT-CNN\",\n",
    "    \"dataset\": \"full_dataset\",\n",
    "    \"datasize\": minified_df.shape[0],\n",
    "    \"epochs\": 50,\n",
    "    \"steps_per_epoch\": tf.data.experimental.cardinality(dataset).numpy(),\n",
    "    \"num_train_steps\": tf.data.experimental.cardinality(dataset).numpy() * 20,\n",
    "    \"num_warmup_steps\": int(0.1 * tf.data.experimental.cardinality(dataset).numpy() * 20),\n",
    "    \"init_lr\": 3e-5,\n",
    "    \"batch_size\" : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_table = [1,2,4,8,16,32,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_table = [3e-6, 1e-5, 1e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_experiment(config, f\"Experiment-bigger-BERT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
